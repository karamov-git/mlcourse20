{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AldI23FZmMX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR, DecisionTreeClassifier as DTC\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "На вход массив чисел $X$ и число бутстрепных выборок $B$. Необходимо реализовать свой бутстреп и найти матожидание и стандартную ошибку у бутстрепных выборок.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 100000\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "42.1, 4.56\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem # ищет SE среднего\n",
    "\n",
    "def get_stats(X: np.array, B:int)->tuple:\n",
    "     '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    return mean, SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 100000\n",
    "\n",
    "mean, se = get_stats(X, B)\n",
    "\n",
    "assert np.abs(mean - 42.1) < 0.05\n",
    "assert np.abs(se - 4.56) < 0.03\n",
    "######################################################\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bias-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается **один** объект $(x, y)$ и список из нескольких **обученных** моделей. \n",
    "\n",
    "Необходимо найти $error$, $bias^2$, $variance$ для данного объекта.\n",
    "\n",
    "Теперь все аккуратно запишем, чтобы не запутаться.\n",
    "\n",
    "* $(x, y)$ - тестировачная выборка\n",
    "* $a_1(\\cdot), \\ldots, a_M(\\cdot)$ - модели (это не обученные на бутстрепе модели, а просто возможные модели из пространства $\\mathbb{A}$, которое мы выбрали)\n",
    "\n",
    "Как настоящие статистики мы можем ~~забить~~ оценить матожидание как среднее.**Это не смешанная модель, а именно оценка матожидания через среднее**\n",
    "$$\\mathbb{E}a(x) = \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)$$\n",
    "\n",
    "**Error** (берем матожидание от квадрата разности)\n",
    "\n",
    "$$error = \\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{M}\\sum_{i=1}^{M}(a_i(x) - y)^2$$\n",
    "\n",
    "**Bias** (заметьте, что возвращаем квадрат bias, а не просто bias)\n",
    "\n",
    "$$bias^2 = \\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\Big(y - \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)\\Big)^2$$  \n",
    "\n",
    "\n",
    "**Variance** (ищем смещенную оценку)\n",
    "\n",
    "$$variance = \\mathbb{D}_{a}a(x)= \\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(a_i(x)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x)\\Big)^2$$\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "x, y = np.array([[0,0,0]]), 0\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=1),  #already fitted estimators\n",
    "              DecisionTreeRegressor(max_depth=5, random_state=1)]\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "error, bias2, var = 3.574, 3.255, 0.319\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bias_variance_decomp(x_test:np.array, y_test:int, estimators:list)->tuple:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "\n",
    "    return error, bias2, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "def generate(n_samples, noise, f):\n",
    "    X = np.linspace(-4, 4, n_samples)\n",
    "    y = f(X)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "######################################################\n",
    "\n",
    "n_train = 150        \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "X, y = generate(n_samples=n_train, noise=noise, f=f)\n",
    "\n",
    "estimators = [DTR(max_depth=2, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=4, random_state=1).fit(X, y)]\n",
    "\n",
    "x, y = np.array([[2]]), 1.5\n",
    "\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([0.108, 0.083, 0.025]), decimal=3)\n",
    "\n",
    "x, y = np.array([[-0.7]]), 0.8\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([0.045, 0.002, 0.043]), decimal=3)\n",
    "\n",
    "######################################################\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10, \n",
    "                       n_targets=1, shuffle=False, random_state=10)\n",
    "\n",
    "estimators = [DTR(max_depth=3, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=5, random_state=1).fit(X, y)]\n",
    "\n",
    "x, y = np.array([[0,0,0]]), 0\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.574, 3.255, 0.319]), decimal=3)\n",
    "\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bias-variance v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь тоже самое, только для нескольких объектов\n",
    "\n",
    "На вход подается тестовая выборка объект $(X_test, y_test)$ и список из нескольких **обученных** моделей. \n",
    "\n",
    "Необходимо найти $error$, $bias^2$, $variance$, $noise$ для данного объекта.\n",
    "\n",
    "$$error = \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}(a_j(x_i) - y_i)^2$$\n",
    "\n",
    "$$bias^2 = \\mathbb{E}_{x,y}\\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(y_i - \\frac{1}{M}\\sum_{j=1}^{M}a_j(x_i)\\Big)^2$$  \n",
    "\n",
    "$$variance = \\mathbb{E}_{x,y}\\mathbb{D}_{a}a(x)= \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}\\Big(a_j(x_i)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x_i)\\Big)^2$$\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "x = np.array([[  0,   0,   0],\n",
    "              [0.1, 0.1, 0.1]])\n",
    "y = np.array([0, 0.1])\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=3), \n",
    "              DecisionTreeRegressor(max_depth=5, random_state=3)]\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "error, bias2, var = 3.399, 3.079, 0.319\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bias_variance_decomp2(x_test:np.array, y_test:np.array, estimators:list)->tuple:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "\n",
    "    return error, bias2, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "def generate(n_samples, noise, f):\n",
    "    X = np.linspace(-4, 4, n_samples)\n",
    "    y = f(X)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "######################################################\n",
    "\n",
    "n_train = 150        \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "X, y = generate(n_samples=n_train, noise=noise, f=f)\n",
    "\n",
    "estimators = [DTR(max_depth=2, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=4, random_state=1).fit(X, y)]\n",
    "\n",
    "x = np.array([[2], [-0.7]]) \n",
    "y = np.array([1.5, 0.8])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          (np.array([0.108, 0.083, 0.025]) + np.array([0.045, 0.002, 0.043])) / 2, decimal=3)\n",
    "\n",
    "######################################################\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10, \n",
    "                       n_targets=1, shuffle=False, random_state=10)\n",
    "\n",
    "estimators = [DTR(max_depth=3, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=5, random_state=1).fit(X, y)]\n",
    "\n",
    "x = np.array([[  0,   0,   0]])\n",
    "y = np.array([0])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.574, 3.255, 0.319]), decimal=3)\n",
    "\n",
    "\n",
    "x = np.array([[  0,   0,   0],\n",
    "              [0.1, 0.1, 0.1]])\n",
    "y = np.array([0, 0.1])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.399, 3.079, 0.319]), decimal=3)\n",
    "\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается некий **необученный** алгоритм регрессии, тренировачная и тестовые выборки и число бутстрепных выборок. Необходимо \n",
    "* бустингом сделать несколько выборок $X_1, \\ldots, X_B$\n",
    "* обучить несколько алгоритмов на этих выборках: $a_1(\\cdot), \\ldots, a_B(\\cdot)$\n",
    "* реализовать бэггинг этого алгоритма и найти собственно предсказания, $error$, $bias^2$ и $variance$.\n",
    "\n",
    "Вот теперь аккуратно. Это - **не матожидание**! Это модель такая.\n",
    "$$a(x) = \\frac{1}{B}\\sum_{b=1}^{B}a_b(x)$$\n",
    "\n",
    "А вот ее матожидание равно для всех алгоритмов:\n",
    "$$\\mathbb{E}_aa(x) = \\mathbb{E}_a\\frac{1}{B}\\sum_{b=1}^{B}a_b(x) = \\mathbb{E}_aa_1(x)$$\n",
    "\n",
    "Но так как теперь, нам нужно посчитать матожидание, мы воспользуемся нашим множеством алгоритмов, обученных на бутстрепе, чтобы получить оценку матожидания единичного алгоритма.\n",
    "\n",
    "$$\\mathbb{E}_aa_1(x) = \\frac{1}{B}\\sum_{j=1}^{B}a_j(x)$$\n",
    "\n",
    "Остальные формулы берутся из предыдущей задачи.\n",
    "\n",
    "P.S.\n",
    "* Так как тут есть вероятности, в целом тесты могут `редко` не взлететь. Перезашлите задачу в этом случае.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "estimator = DecisionTreeRegressor(max_depth=1)\n",
    "X_train = np.array([[1, 1], [2, 2]])\n",
    "y_train = np.array([1, 2])\n",
    "X_test  = np.array([[0, 0], [4, 4], [8, 8]])\n",
    "y_test  = np.array([0, 4, 8])\n",
    "\n",
    "B = 10\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3.708, 6.016])\n",
    "error  = 3.5 \n",
    "bias^2 = 0.1\n",
    "var    = 3.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bagging(estimator, X_train, y_train, X_test, y_test, boot_count):\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "\n",
    "    return y_pred, loss, bias, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "estimator = DTR(max_depth=2)\n",
    "X_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\n",
    "y_train = np.array([0, 1, 5, 8, 10])\n",
    "X_test  = np.array([[4, 4], [6, 6]])\n",
    "y_test  = np.array([4, 6])\n",
    "\n",
    "B = 1000\n",
    "\n",
    "y_pred, loss, bias, var = bagging(estimator, X_train, y_train, X_test, y_test, boot_count=B)\n",
    "\n",
    "# Да я в курсе что очень грубые ограничения, просто пример игрушечный на таком малом количестве данных\n",
    "assert_array_almost_equal(y_pred, np.array([4, 6]), decimal=0)\n",
    "\n",
    "assert_almost_equal(loss, 3.7, decimal=0) \n",
    "assert_almost_equal(bias, 0.1, decimal=1) \n",
    "assert_almost_equal(var,  3.7, decimal=0) \n",
    "\n",
    "######################################################\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=7)\n",
    "\n",
    "y_pred, loss, bias, var = bagging(\n",
    "        tree, X_train, y_train, X_test, y_test, boot_count=200)\n",
    "\n",
    "assert_almost_equal(loss, 32, decimal=0) \n",
    "assert_almost_equal(bias, 14, decimal=0) \n",
    "assert_almost_equal(var,  18, decimal=0) \n",
    "\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) RF Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось переделать чуток предыдущую задачу в `RandomForest`. \n",
    "Но теперь мы наконец попробуем классификацию. (Пока только бинарную)\n",
    "\n",
    "План\n",
    "* Также делаем бутстрепные выборки\n",
    "* Бэггинг теперь будет только по деревьям классификации\n",
    "* Будем передавать параметр `n_estimators`, `max_depth` и `max_features`\n",
    "\n",
    "Как выбирать ответ в задаче классификации?\n",
    "* Для каждого внутреннего дерева решений находим веротности обоих классов для каждого объекта $X_test$:\n",
    "  * Вызываем `predict_proba` у `DecisionTreeClassifier`\n",
    "* Усредняем вероятности класса и объекта по деревьям:\n",
    "  * $P(n_{class}=d, object=x_k) = \\frac{1}{B}\\sum_{i=1}^{B}P(n_{class}=d, object=x_k, tree=b_i)$\n",
    "* Для каждого объекта выбираем тот класс, у которого выше вероятность\n",
    "\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6]])\n",
    "y_test  = np.array([0, 1])\n",
    "\n",
    "B = 1000\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRF():\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None):\n",
    "        self.estimators_ = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X_test)-> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6]])\n",
    "y_test  = np.array([0, 1])\n",
    "\n",
    "B = 1000\n",
    "\n",
    "y_pred_my = MyRFC(n_estimators = 100, max_depth=3).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert_array_almost_equal(y_pred_my, np.array([0, 1]))\n",
    "######################################################\n",
    "from random import gauss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_samples = 1000\n",
    "theta = np.linspace(0, 2*np.pi, num_samples)\n",
    "\n",
    "r1 = 1\n",
    "r2 = 2\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "circle = np.hstack([np.cos(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8), \n",
    "                    np.sin(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8)])\n",
    "lil = r1 * circle\n",
    "big = r2 * circle\n",
    "X = np.vstack([lil, big])\n",
    "y = np.hstack([np.zeros(num_samples), np.ones(num_samples)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "y_pred_my = MyRFC(n_estimators = 100, \n",
    "                  max_depth=1).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert accuracy_score(y_pred_my, y_test) > 0.85\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто верните отсортированный массив важности фич, полученные из обученного RandomForest. Фичи нумеруются с 1.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
    "y = np.array([0,0,1,1])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "features= np.array([1, 2])\n",
    "importance = np.array([0.75, 0.25])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X, y):\n",
    "        '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    return features, importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
    "y = np.array([0,0,1,1])\n",
    "\n",
    "f, i = feature_importance(X, y)\n",
    "\n",
    "assert_array_equal(f , np.array([1, 2]))\n",
    "assert i[0] > 0.74\n",
    "######################################################\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=4,\n",
    "                           n_informative=2,\n",
    "                           shuffle=False, \n",
    "                           random_state=10)\n",
    "\n",
    "n = 10\n",
    "a = np.zeros((n, X.shape[1]))\n",
    "for i in range(n):\n",
    "    a[i], _ = feature_importance(X, y) \n",
    "\n",
    "assert_array_equal(np.round(a.mean(axis=0)), np.array([2,3,4,1]))\n",
    "\n",
    "######################################################\n",
    "print('Well Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
